{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gileshall/ML-Biology-Notebooks/blob/main/Taxonomy_with_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycnfVm42gZnH"
      },
      "source": [
        "# Cat or Dog ... DNA?\n",
        "\n",
        "Kaggle first announced their [dog-vs-cat](https://www.kaggle.com/c/dogs-vs-cats) competition in 2013, asking participants to develop a computer vision model that would classify images of cats versus dogs.  Since its debut nine years ago, there are now thousands of solutions available, some which achieve near perfect accuracy.\n",
        "\n",
        "The first build of the human genome was published in 2003, and since then, there are now thousands of published genomes for different species from around the world, including dogs and cats.  If we can train a model to distinguish between the image of a dog versus a cat, can we also train a model to classify between dog DNA and cat DNA? Or, between any number of species?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wg0kRuWGRLH"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers biopython datasets pyfaidx tqdm colored wandb joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training Parameters\n",
        "\n",
        "#@markdown Comma seperated list of organisms to classify\n",
        "organisms = 'cat, dog' #@param {type:\"string\"}\n",
        "organisms = sorted(set(map(str.strip, organisms.split(','))))\n",
        "\n",
        "#@markdown How many training examples to sample from each genome?\n",
        "training_samples_per_genome =  10000#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown How many evaluation examples to sample from each genome?\n",
        "eval_samples_per_genome =  1000#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Exclude low-complexity regions from the training set?\n",
        "#@markdown - none: do not exclude any region\n",
        "#@markdown - masked: exclude soft-masked regions (train exclusively on upper case bases)\n",
        "#@markdown - unmasked: include ONLY soft-masked regions (train exclusively on lower case bases)\n",
        "exclude_region = 'unmasked' #@param ['masked', 'unmasked', 'none']\n",
        "\n",
        "#@markdown DNABERT kmer length?\n",
        "klen = 6#@param [3, 4, 5, 6]\n",
        "\n",
        "#@markdown Do you have a weights and biases account?\n",
        "#@markdown - Yes: you will be prompted for your wandb API key before training starts.\n",
        "#@markdown - No: wandb will automatically post your results anonymous\n",
        "wandb_account_flag = False #@param {type:\"boolean\"}\n",
        "wandb_anon = 'must' if not wandb_account_flag else 'never'\n",
        "\n",
        "import io\n",
        "import re\n",
        "import os\n",
        "import gzip\n",
        "import json\n",
        "import random\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "from uuid import uuid4\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "import wandb\n",
        "import datasets\n",
        "import numpy as np\n",
        "from colored import fg, bg, attr, set_tty_aware\n",
        "import pandas as pd\n",
        "import requests\n",
        "from joblib import Memory\n",
        "from tqdm import tqdm\n",
        "from Bio import Entrez\n",
        "from pyfaidx import Fasta\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers.utils.logging import set_verbosity_error, set_verbosity_info\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "memory = Memory('cache', verbose=False)\n",
        "\n",
        "project_name = \"taxonomy_with_transformers\"\n",
        "total_samples = (training_samples_per_genome + eval_samples_per_genome) * len(organisms)\n",
        "run_name = f'{str.join(\"_\", organisms)}_{total_samples // 1000}K_exclude_{exclude_region}'\n",
        "path_output = f'{run_name}/output'\n",
        "path_dataset = f'{run_name}/dataset'\n",
        "path_genomes = f'genomes'\n",
        "\n",
        "Entrez.email = f'{str(uuid4())}@example.com'\n",
        "\n",
        "os.environ['FORCE_COLOR'] = str(3)\n",
        "GRN = fg('green')\n",
        "RED = fg('red')\n",
        "BLU = fg('blue')\n",
        "RST = attr('reset')\n",
        "N_PROC = cpu_count()\n",
        "\n",
        "def load_model(klen=6, id2label=None):\n",
        "    set_verbosity_error()\n",
        "    msg = f'{GRN}Loading DNABERT (klen={klen}, id2label={id2label}){RST}'\n",
        "    print(msg)\n",
        "    assert klen >= 3 and klen <= 6\n",
        "    model_path = f\"armheb/DNA_bert_{klen}\"\n",
        "    label2id = dict(zip(id2label.values(), id2label.keys()))\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        model_path, num_labels=len(id2label),\n",
        "        id2label=id2label, label2id=label2id\n",
        "    )\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=False)\n",
        "    return (model, tokenizer)\n",
        "\n",
        "@memory.cache\n",
        "def esearch(block_size=1000, limit=None, **kw):\n",
        "    req = Entrez.esearch(**kw, retmax=1)\n",
        "    res = Entrez.read(req)\n",
        "    total = res['Count'] = int(res['Count'])\n",
        "    if limit:\n",
        "        total = min(total, limit)\n",
        "    block_size = min(block_size, total)\n",
        "    while len(res['IdList']) < total:\n",
        "        pos = len(res['IdList'])\n",
        "        assert len(res['IdList']) == len(set(res['IdList']))\n",
        "        req = Entrez.esearch(**kw, retstart=pos, retmax=block_size)\n",
        "        res['IdList'] += Entrez.read(req)['IdList']\n",
        "    return res\n",
        "\n",
        "@memory.cache\n",
        "def esummary(**kw):\n",
        "    req = Entrez.esummary(**kw)\n",
        "    res = Entrez.read(req)\n",
        "    return res\n",
        "\n",
        "def search_for_genome(query=None):\n",
        "    re_acc = re.compile('^(G..)_(\\d\\d\\d)(\\d\\d\\d)(\\d\\d\\d).*$')\n",
        "    url_base = 'https://ftp.ncbi.nlm.nih.gov/genomes/all/'\n",
        "\n",
        "    idlist = None\n",
        "    for query_type in ('Organism', 'Accession'):\n",
        "        term = f'{query}[{query_type}]'\n",
        "        res = esearch(db='assembly', term=term)\n",
        "        if res['Count'] > 0:\n",
        "            idlist = res['IdList']\n",
        "            break\n",
        "    if idlist is None:\n",
        "        return None\n",
        "    asm_list = []\n",
        "    for asm_id in idlist:\n",
        "        ret = esummary(db='assembly', id=asm_id)\n",
        "        asm_info = {'id': asm_id, 'query': query}\n",
        "        summary = ret['DocumentSummarySet']['DocumentSummary'][0]\n",
        "        sub_date = summary.get('SubmissionDate')\n",
        "        asm_info['submission_date'] = datetime.strptime(sub_date, \"%Y/%m/%d %H:%M\")\n",
        "        asm_info['accession'] = summary.get('AssemblyAccession')\n",
        "        asm_info['name'] = summary.get('AssemblyName')\n",
        "        asm_info['status'] = summary.get('AssemblyStatus')\n",
        "        asm_info['organism'] = summary.get('Organism')\n",
        "        asm_info['taxid'] = summary.get('SpeciesTaxid')\n",
        "        asm_info['partial'] = (str(summary.get('PartialGenomeRepresentation', 'true').lower()) == 'true')\n",
        "        if asm_info['partial']:\n",
        "            continue\n",
        "        if asm_info['status'] != 'Chromosome':\n",
        "            continue\n",
        "        build_key = f'{asm_info[\"accession\"]}_{asm_info[\"name\"]}'\n",
        "        asm_info['build_key'] = build_key\n",
        "        ftp_url = summary.get('FtpPath_GenBank')\n",
        "        url = ftp_url.replace('ftp://', 'https://')\n",
        "        url = '/'.join([url, f'{build_key}_genomic.fna.gz'])\n",
        "        asm_info['url'] = url\n",
        "        asm_list.append(asm_info)\n",
        "    asm_list = sorted(asm_list, key=lambda it: it['submission_date'])\n",
        "    asm_pick = asm_list[-1]\n",
        "    return asm_pick\n",
        "\n",
        "def find_all_genomes(genomes, path_genomes=None):\n",
        "    os.makedirs(path_genomes, exist_ok=True)\n",
        "    msg = f'{GRN}Searching for genomes:{RST}'\n",
        "    print(msg)\n",
        "\n",
        "    ret = {}\n",
        "    for (label_id, name) in enumerate(genomes):\n",
        "        info = search_for_genome(name)\n",
        "        if info is None:\n",
        "            msg = f\"{RED}Genome not found for '{name}'{RST}\"\n",
        "            raise KeyError(msg)\n",
        "        msg = '  - {query} ({organism}), Assembly accession {accession}'.format(**info)\n",
        "        print(msg)\n",
        "        ret[name] = {'info': info, 'label_id': label_id, 'label_name': str(info['organism'])}\n",
        "    return ret\n",
        "\n",
        "def load_genome(info=None, path_genomes=None):\n",
        "    os.makedirs(path_genomes, exist_ok=True)\n",
        "    genome_fn = '{build_key}_genome.fa'.format(**info)\n",
        "    genome_fn = os.path.join(path_genomes, genome_fn)\n",
        "    if not os.path.exists(genome_fn):\n",
        "        msg = \"  - Downloading '{query}' genome ({organism})\".format(**info)\n",
        "        print(msg)\n",
        "        url = info['url']\n",
        "        with requests.get(url, stream=True) as resp:\n",
        "            fa_str = gzip.decompress(resp.content)\n",
        "        with open(genome_fn, 'wb') as fh:\n",
        "            fh.write(fa_str)\n",
        "    msg = \"  - Loading '{query}' genome ({organism})\".format(**info)\n",
        "    print(msg)\n",
        "    return Fasta(genome_fn)\n",
        "\n",
        "def sample_genome(genome, samples_per_genome=None, sample_len=None, exclude_region=False, sequence_prefix=('CM',)):\n",
        "    # NB: set math stipulates equality, not subset.  In other words, no matter\n",
        "    # the exclusion, the example must contain ALL bases\n",
        "    if exclude_region == 'masked':\n",
        "        msg_exclude = 'excluding masked regions'\n",
        "        ok_nucs = set('AGTC')\n",
        "        f_func = lambda it: set(str(seq)) == ok_nucs\n",
        "    elif exclude_region == 'unmasked':\n",
        "        msg_exclude = 'excluding unmasked regions'\n",
        "        ok_nucs = set('agtc')\n",
        "        f_func = lambda it: set(str(seq)) == ok_nucs\n",
        "    elif exclude_region == 'none':\n",
        "        msg_exclude = ''\n",
        "        ok_nucs = set('AGTC')\n",
        "        f_func = lambda it: set(str(seq).upper()) == ok_nucs\n",
        "    else:\n",
        "        raise ValueError(exclude_region)\n",
        "    chr_filter = lambda nm: any([nm.startswith(pre) for pre in sequence_prefix])\n",
        "    (seq_names, seq_lens) = zip(*[(rec.name, len(rec)) for rec in genome if chr_filter(rec.name)])\n",
        "\n",
        "    seq_total_len = sum(seq_lens)\n",
        "    sample_total_len = samples_per_genome * sample_len\n",
        "    genome_sample_percent = sample_total_len / seq_total_len * 100\n",
        "    msg = f'  - Sampling {sample_total_len / 1e6:.02f} megabases from {seq_total_len / 1e6:.02f} megabase genome ({genome_sample_percent:.02f}%) {msg_exclude}'\n",
        "    print(msg)\n",
        "    if seq_total_len == 0:\n",
        "        raise ValueError(\"Empty genome\")\n",
        "    seq_weights = [slen / seq_total_len for slen in seq_lens]\n",
        "    seq_map = {seq_name: str(genome[seq_name]) for seq_name in seq_names}\n",
        "    for cnt in tqdm(range(samples_per_genome), total=samples_per_genome):\n",
        "        while True:\n",
        "            seq_name = random.choices(seq_names, seq_weights)[0]\n",
        "            seq = seq_map[seq_name]\n",
        "            if sample_len > (len(seq) - sample_len):\n",
        "                continue\n",
        "            start = random.randint(0, len(seq) - sample_len)\n",
        "            end = start + sample_len\n",
        "            seq = seq[start:end]\n",
        "            if f_func(seq):\n",
        "                name = f'{seq_name}:{start}-{end}'\n",
        "                break\n",
        "        yield {'seq': str(seq), 'name': name}\n",
        "\n",
        "def kmerize(seq=None, klen=6):\n",
        "    kmers = [seq[i:i + klen] for i in range(len(seq) - klen + 1)]\n",
        "    return kmers\n",
        "\n",
        "def tokenize(it, klen=None, kmerize=None, tokenizer=None):\n",
        "    seq = it['seqs'].upper()\n",
        "    vocab = tokenizer.vocab\n",
        "    max_length = tokenizer.model_max_length\n",
        "    kmers = kmerize(seq=seq, klen=klen)\n",
        "    inp = ['[CLS]'] + kmers + ['[SEP]']\n",
        "    toks = list(map(vocab.__getitem__, inp))\n",
        "    toks += [0] * (max_length - len(toks))\n",
        "    ret = {\n",
        "        \"input_ids\": np.array(toks, dtype=int),\n",
        "        \"attention_mask\": np.array(toks != 0, dtype=int),\n",
        "        \"token_type_ids\": np.zeros_like(toks, dtype=int),\n",
        "    }\n",
        "    return ret\n",
        "\n",
        "def build_dataset(\n",
        "        all_genomes=None,\n",
        "        training_samples_per_genome=None,\n",
        "        eval_samples_per_genome=None,\n",
        "        sample_len=None,\n",
        "        klen=None,\n",
        "        tokenizer=None,\n",
        "        kmerize=None,\n",
        "        exclude_region=False,\n",
        "        path_genomes=None\n",
        "    ):\n",
        "\n",
        "    total_samples_per_genome = training_samples_per_genome + eval_samples_per_genome\n",
        "    sample_len = sample_len or (klen + tokenizer.model_max_length - 3)\n",
        "\n",
        "    ds_list = []\n",
        "    print(f'\\n{GRN}Building training dataset:{RST}')\n",
        "    for key in all_genomes:\n",
        "        label_id = all_genomes[key]['label_id']\n",
        "        label_name = all_genomes[key]['label_name']\n",
        "        genome_info = all_genomes[key]['info']\n",
        "        genome = load_genome(genome_info, path_genomes=path_genomes)\n",
        "        # msg = \"  - Randomly sampling '{query}' genome ({organism})\".format(**genome_info)\n",
        "        # print(msg)\n",
        "        samples = sample_genome(\n",
        "            genome=genome,\n",
        "            sample_len=sample_len,\n",
        "            samples_per_genome=total_samples_per_genome,\n",
        "            exclude_region=exclude_region\n",
        "        )\n",
        "        samples = list(samples)\n",
        "        names = [smp['name'] for smp in samples]\n",
        "        seqs = [smp['seq'] for smp in samples]\n",
        "        labels = [label_id] * len(seqs)\n",
        "\n",
        "        dataset = datasets.Dataset.from_dict(dict(seqs=seqs, names=names, labels=labels))\n",
        "        dataset = dataset.train_test_split(train_size=training_samples_per_genome)\n",
        "        ds_list.append(dataset)\n",
        "        print()\n",
        "\n",
        "    print(f'{GRN}Tokenizing, splitting and shuffling datasets{RST}')\n",
        "    ds_dict = {}\n",
        "    for key in ('train', 'test'):\n",
        "        print(f'  - split={key}')\n",
        "        _ds_list = [ds[key] for ds in ds_list]\n",
        "        dataset = datasets.concatenate_datasets(_ds_list).shuffle()\n",
        "        dataset = dataset.map(\n",
        "            tokenize,\n",
        "            fn_kwargs={'kmerize': kmerize, 'klen': klen, 'tokenizer': tokenizer},\n",
        "            new_fingerprint=f'{klen}_tokenizer',\n",
        "            num_proc=N_PROC,\n",
        "        )\n",
        "        ds_dict[key] = dataset\n",
        "    dataset = datasets.DatasetDict(ds_dict)\n",
        "    return dataset\n",
        "\n",
        "def build_experiment(\n",
        "        organisms=None,\n",
        "        klen=None,\n",
        "        training_samples_per_genome=None,\n",
        "        eval_samples_per_genome=None,\n",
        "        path_dataset=None,\n",
        "        **kw,\n",
        "    ):\n",
        "\n",
        "    all_genomes = find_all_genomes(organisms, path_genomes=path_genomes)\n",
        "    print()\n",
        "    id2label = {idx: all_genomes[org]['label_name'] for (idx, org) in enumerate(organisms)}\n",
        "    (model, tokenizer) = load_model(klen=klen, id2label=id2label)\n",
        "    if os.path.isdir(path_dataset):\n",
        "        msg = f'\\n{GRN}Loading dataset from {path_dataset}{RST}'\n",
        "        print(msg)\n",
        "        dataset = datasets.load_from_disk(path_dataset)\n",
        "    else:\n",
        "        dataset = build_dataset(\n",
        "            all_genomes=all_genomes,\n",
        "            training_samples_per_genome=training_samples_per_genome,\n",
        "            eval_samples_per_genome=eval_samples_per_genome,\n",
        "            klen=klen,\n",
        "            tokenizer=tokenizer,\n",
        "            **kw\n",
        "        )\n",
        "        dataset.save_to_disk(path_dataset)\n",
        "\n",
        "    print(f'\\n{BLU}{dataset}{RST}\\n')\n",
        "    return (model, tokenizer, dataset)\n"
      ],
      "metadata": {
        "id": "TZpiNsq4-yzq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the classification task"
      ],
      "metadata": {
        "id": "5Fu73D0Lv08l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5efKnSYptwTx"
      },
      "outputs": [],
      "source": [
        "epochs = 4\n",
        "warmup_steps = 500\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 4 * train_batch_size\n",
        "\n",
        "(model, tokenizer, dataset) = build_experiment(\n",
        "    organisms=organisms,\n",
        "    klen=klen,\n",
        "    training_samples_per_genome=training_samples_per_genome,\n",
        "    eval_samples_per_genome=eval_samples_per_genome,\n",
        "    exclude_region=exclude_region,\n",
        "    kmerize=kmerize,\n",
        "    path_dataset=path_dataset,\n",
        "    path_genomes=path_genomes\n",
        ")\n",
        "label_names = [\n",
        "    model.config.id2label[idx] for idx in range(len(model.config.id2label))\n",
        "]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=path_output,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_strategy='steps',\n",
        "    logging_first_step=True,\n",
        "    logging_steps=100,\n",
        "    report_to='wandb',\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=eval_batch_size,\n",
        "    disable_tqdm=False,\n",
        ")\n",
        "\n",
        "def compute_confusion_matrix(truth=None, preds=None, label_names=None):\n",
        "    imgfn = \"confusion_matrix.png\"\n",
        "    cm = confusion_matrix(truth, preds)\n",
        "    cm = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
        "    cm = cm.plot()\n",
        "    cm.figure_.savefig(imgfn, bbox_inches='tight', pad_inches=0, dpi=120)\n",
        "    log = {\"eval/confusion_matrix\": wandb.Image(imgfn)}\n",
        "    wandb.log(log)\n",
        "\n",
        "def compute_classification_report(truth=None, preds=None, label_names=None):\n",
        "    rpt = classification_report(truth, preds, target_names=label_names, zero_division=0, output_dict=True)\n",
        "    res = {}\n",
        "    for top_cat in rpt:\n",
        "        item = rpt[top_cat]\n",
        "        if type(item) is dict:\n",
        "            for bot_cat in item:\n",
        "                key = f'{top_cat}_{bot_cat}'\n",
        "                res[key] = item[bot_cat]\n",
        "        else:\n",
        "            res[top_cat] = item\n",
        "    for key in list(res.keys()):\n",
        "        if 'weight' in key:\n",
        "            del res[key]\n",
        "    return res\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    truth = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    log = dict()\n",
        "    calls = [compute_confusion_matrix,  compute_classification_report]\n",
        "    for call in calls:\n",
        "        ret = call(truth, preds, label_names=label_names)\n",
        "        if type(ret) is dict:\n",
        "            log.update(ret)\n",
        "    return ret\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "with wandb.init(project=project_name, name=run_name, anonymous=wandb_anon):\n",
        "    trainer.evaluate()\n",
        "    training_output = trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "name": "Taxonomy with Transformers",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}